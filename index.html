<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="SpinBench is a cognitively inspired benchmark that decomposes perspective taking into fine-grained tasks, which reveals systematic weaknesses in 37 VLMs and highlights the need for structured diagnostics to advance spatial reasoning.">
  <meta name="keywords" content="VLM, Spatial Reasoning, Perspective">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="./static/images/spinning-top.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <!-- <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a> -->

        <!-- <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div>
      </div> -->

      </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning
              in VLMs</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://zhangyuyou-10.github.io/">Yuyou Zhang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ZB99OtYAAAAJ&hl=en">Radu Corcodel</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=a_M5FlgAAAAJ&hl=en">Chiori
                  Hori</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=XVUCn40AAAAJ&hl=en">Anoop Cherian</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=z7tPc9IAAAAJ&hl=en">Ding Zhao</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University,</span>
              <span class="author-block"><sup>2</sup>Mitsubishi Electric Research Labs</span>
            </div>

            <div class="column has-text-centered">

              <!-- ICLR 2026 Acceptance Badge -->
              <div style="margin-bottom: 1.25rem;">
                <span style="
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 28px;
      background-color: #fff0f0;
      border: 2.5px solid #e03030;
      border-radius: 50px;
      font-size: 1.1rem;
      font-weight: 800;
      color: #e03030;
    ">
                  ðŸ”¥ Accepted by ICLR 2026
                </span>
              </div>

              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
      <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper</span>
      </a>
    </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2509.25390" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ZhangYuyou-10/SpinBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/YuyouZhang/SpinBench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/overview_example.png" alt="geometric reasoning" width="99%" />
        <p> Overview of SPINBENCH task design across seven task groups. Representative subtasks
          are illustrated for each group with simplified question wording for clarity. In the released benchmark,
          all queries include explicit frame-of-reference definitions to avoid ambiguity. Human face data are sourced
          from the <a href="https://www.epfl.ch/labs/cvlab/data/data-stereoface/">Stereo Face Database</a> and are
          licensed for research use only.
        </p>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
          free-viewpoint
          portraits.
        </h2>
      </div>
    </div>
  </section>
  


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present SpinBench, a cognitively grounded diagnostic benchmark for evaluating spatial reasoning in
              vision language models (VLMs).
              SpinBench is designed around the core challenge of spatial reasoning: perspective taking, the ability to
              reason about how scenes and object relations change under viewpoint transformation.
              Since perspective taking requires multiple cognitive capabilities, such as recognizing objects across
              views, relative positions grounding, and mentally simulating transformations, SpinBench introduces a set
              of fine-grained diagnostic categories.
              Our categories target translation, rotation, object relative pose, and viewpoint change, and are
              progressively structured so that single-object simpler tasks scaffold toward the most demanding
              multi-object perspective-taking setting.
              We evaluate 37 state-of-the-art VLMs, both proprietary and open source.
              Results reveal systematic weaknesses: strong egocentric bias, poor rotational understanding, and
              inconsistencies under symmetrical and syntactic reformulations.
              Scaling analysis shows both smooth improvements and emergent capabilities.
              While human subjects achieve high accuracy (91.2%), task difficulty as measured by human response time
              shows strong correlation with VLM accuracy, indicating that SpinBench captures spatial reasoning
              challenges shared across humans and VLMs.
              We believe SpinBench provides critical insights into spatial reasoning in VLMs and highlights key gaps in
              their ability to reason about physical space.
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Evaluations on 37 VLMs
          </h2>
          <img src="static/images/vlm_benchmark_cohens_kappa_heatmap_bottom_v2.png" alt="eval-heatmap"
            style="max-width: 80%;" />
          <div class="content has-text-justified">
            <p>
              In the figure below, Cohen's kappa values (Îº) measure chance-adjusted performance, where Îº=0 indicates
              chance-level and Îº=1 perfect accuracy. Results is shown across 23 grouped task variants under 7 spatial
              reasoning categories.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <!-- Inconsistencies in logically equivalent spatial queries. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Inconsistencies in logically equivalent spatial queries</h3>
        <img src="static/images/accuracy_consistency_analysis.png" alt="consistency_analysis" style="max-width: 80%;" />
        <div class="content has-text-justified">
          <p>
            Many models are inconsistent in logically equivalent spatial tasks. While InternVL3-38B achieves 95.7%
            consistency, many fall below 30%. A strong correlation (r = 0.874, p < 0.05) between accuracy and
              consistency. </p>
        </div>
      </div>
    </div>
    <!--/ Inconsistencies in logically equivalent spatial queries. -->
  </section>

  <section class="section">
    <!-- Biased perspective. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Biased perspective</h3>
        <div class="content has-text-justified">
          <p>
            Models show a strong egocentric bias in dynamic rotation tasks, even when asked to adopt other viewpoints.
            Top performers on egocentric tasks do poorly on allocentric ones, likely due to training data favoring
            first-person views.
          </p>
        </div>
      </div>
    </div>
    <!--/ Biased perspective. -->
  </section>


  <section class="section">
    <!-- Scaling laws and emergent capability -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Scaling laws and emergent capability</h3>
        <img src="static/images/scaling_laws_benchmark.png" alt="human_analysis" style="max-width: 90%;" />
        <div class="content has-text-justified">
          <p>
            Performance improves with model size but varies by task. Object relation grounding improves gradually, while
            identity matching shows a sharp jump once models exceed 7B parameters. These trends reflect emergent
            abilities and reveal gaps between small and large models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Scaling laws and emergent capability -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <!-- Visual failures or linguistic failures. -->
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Visual failures or linguistic failures</h2>
            <img src="static/images/linguistic_spatial_reasoning_analysis.png" alt="linguistic_analysis"
              style="max-width: 95%;" />
            <p>
              Perspective-taking tasks test spatial reasoning across viewpoints. Even when spatial relations are clearly
              described (with information abstracted in the premise, no visual input needed), many models fail, showing
              that reasoning errors persist even in purely
              linguistic tasks. Top performing models perform also perform well on linguistic tasks.
            </p>
          </div>
        </div>
        <!--/ Visual failures or linguistic failures. -->

        <!-- Human response time -->
        <div class="column">
          <h2 class="title is-4">Human response time and VLM accuracy</h2>
          <div class="columns is-centered">
            <div class="column content">
              <img src="static/images/vlm_human_correlation_subtype_wo_errorbar.png" alt="human_analysis"
                style="max-width: 95%;" />
              <p>
                We conducted human evaluation with twelve subjects to establish performance baselines and validate task
                difficulty.
                Tasks taking longer for humans also result in lower VLM accuracy, showing a correlation (r = â€“0.54, p <
                  0.05). SpinBench captures captures spatial reasoning challenges shared across humans and VLMs, as a
                  diagnostic tool.</p>
            </div>

          </div>
        </div>
      </div>
      <!--/ Human response time -->
    </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2025spinbench,
  title={SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs},
  author={Zhang, Yuyou and Corcodel, Radu and Hori, Chiori and Cherian, Anoop and Zhao, Ding},
  journal={arXiv preprint arXiv:2509.25390},
  year={2025}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a
              href="https://mathvista.github.io/">MathVista</a> licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </footer>

</body>

</html>